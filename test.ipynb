{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Game import Game\n",
    "import sys \n",
    "sys.path.append(\"Othello\")\n",
    "from othello_game import OthelloGame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "game = OthelloGame(n=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0,  0,  0,  0,  0,  0,  0,  0],\n",
       "       [ 0,  0,  0,  0,  0,  0,  0,  0],\n",
       "       [ 0,  0,  0,  0,  0,  0,  0,  0],\n",
       "       [ 0,  0,  0, -1,  1,  0,  0,  0],\n",
       "       [ 0,  0,  0,  1, -1,  0,  0,  0],\n",
       "       [ 0,  0,  0,  0,  0,  0,  0,  0],\n",
       "       [ 0,  0,  0,  0,  0,  0,  0,  0],\n",
       "       [ 0,  0,  0,  0,  0,  0,  0,  0]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "game.getInitBoard()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "board = game.getInitBoard()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(board)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0,  0,  0,  0,  0,  0,  0,  0],\n",
       "       [ 0,  0,  0,  0,  0,  0,  0,  0],\n",
       "       [ 0,  0,  0,  0,  0,  0,  0,  0],\n",
       "       [ 0,  0,  0, -1,  1,  0,  0,  0],\n",
       "       [ 0,  0,  0,  1, -1,  0,  0,  0],\n",
       "       [ 0,  0,  0,  0,  0,  0,  0,  0],\n",
       "       [ 0,  0,  0,  0,  0,  0,  0,  0],\n",
       "       [ 0,  0,  0,  0,  0,  0,  0,  0]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "board"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "player_a = np.maximum(board, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "player_b = board.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "player_b[player_b > 0] = 0 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "player_b=player_b*-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 1, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 1, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0]])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "player_b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0,  0,  0,  0,  0,  0,  0,  0],\n",
       "       [ 0,  0,  0,  0,  0,  0,  0,  0],\n",
       "       [ 0,  0,  0,  0,  0,  0,  0,  0],\n",
       "       [ 0,  0,  0, -1,  1,  0,  0,  0],\n",
       "       [ 0,  0,  0,  1, -1,  0,  0,  0],\n",
       "       [ 0,  0,  0,  0,  0,  0,  0,  0],\n",
       "       [ 0,  0,  0,  0,  0,  0,  0,  0],\n",
       "       [ 0,  0,  0,  0,  0,  0,  0,  0]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "board"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "\n",
    "board_tensor = torch.tensor(board, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 8])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "board_tensor.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want a stack of (MT+L) NxN planes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = []\n",
    "for a in range(5): \n",
    "    tmp.append(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Author: Aryaman Pandya\n",
    "File contents: Convolutional neural network that takes in board state \n",
    "and outputs expected value \n",
    "\"\"\"\n",
    "\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class OthelloNN(nn.Module):\n",
    "    '''\n",
    "    Convolutional neural network used in the AlphaZero implementation scaled \n",
    "    for the dimensions of the othello game. \n",
    "    '''\n",
    "    def __init__(self) -> None:\n",
    "        '''\n",
    "        Initialization of the neural network graph. \n",
    "        Contains a common body that includes 4 sequential Conv2D operations \n",
    "        followed by batch normalization and ReLU. \n",
    "        Contains two separate heads for policy and value estimation as specified \n",
    "        in the original DeepMind paper supplemental materials. \n",
    "        '''\n",
    "\n",
    "        super().__init__()\n",
    "        #we expect an input of dimensionality 8 x 8 x 7 following conventions from the paper:\n",
    "        #N x N -> 8 x 8. M = 2, T = 3, L =1 (who's playing)\n",
    "\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=7, out_channels=128, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.conv2 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=128, out_channels=128, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.conv3 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=128, out_channels=128, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.conv4 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=128, out_channels=128, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "        self.policy_head = nn.Sequential(nn.Linear(8192, 64), nn.Softmax(dim=1))\n",
    "\n",
    "        self.value_head_conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=128, out_channels=1, kernel_size=1),\n",
    "            nn.BatchNorm2d(1),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "        self.value_head_linear = nn.Sequential(\n",
    "            nn.Linear(64, 64), nn.ReLU(), nn.Linear(64, 1), nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, state) -> tuple[np.array, int]:\n",
    "        '''\n",
    "        Forward pass for the nn graph\n",
    "\n",
    "        Args:\n",
    "            param1: self\n",
    "            param2: state- game state at time of evaluation \n",
    "\n",
    "        Returns:\n",
    "            pi (torch.tensor): policy pi[a|s]\n",
    "            val (float32): scalar value estimate from input state \n",
    "        '''\n",
    "\n",
    "        s = self.conv1(state)\n",
    "        s = self.conv2(s)\n",
    "        s = self.conv3(s)\n",
    "        s = self.conv4(s)\n",
    "\n",
    "        pi = self.policy_head(s)\n",
    "        s = self.value_head_conv(s)\n",
    "        val = self.value_head_linear(s)\n",
    "\n",
    "        return pi, val\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_history_frames(history: np.ndarray, new_frame: np.ndarray, m: int, history_length: int): \n",
    "    \"\"\"\n",
    "    Updates the history of game boards with a new frame.\n",
    "\n",
    "    Shifts existing frames in history and adds the new frame at the end.\n",
    "\n",
    "    Args:\n",
    "        history (np.ndarray): Game frame history (NxNx(MT+L))\n",
    "        new_frame (np.ndarray): 2D array representing new game state.\n",
    "        m (int): Number of channels per frame.\n",
    "        history_length (int): Number of frames in history.\n",
    "\n",
    "    Returns:\n",
    "        None: Updates 'history' array in place.\n",
    "    \"\"\"\n",
    "    board_player_1, board_player_2 = split_player_boards(new_frame)\n",
    "    history[m*(history_length-1):, :, :] = history[:, :, m:]\n",
    "    new_frames = np.stack([board_player_1, board_player_2], axis=0)\n",
    "    history[m*(history_length-1):m*history_length:, :] = new_frames\n",
    "\n",
    "\n",
    "def add_player_information(board_tensor: np.ndarray, current_player: int):\n",
    "    \"\"\"\n",
    "    Adds a feature plane indicating the current player.\n",
    "\n",
    "    Args:\n",
    "        board_tensor (np.ndarray): The tensor representing the game state.\n",
    "        current_player (int): The current player (e.g., 0 or 1).\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: Updated board tensor with the player information added.\n",
    "    \"\"\"\n",
    "    # Assuming the last channel is for the current player information\n",
    "    player_plane = np.full((board_tensor.shape[0], board_tensor.shape[1]), current_player)\n",
    "    board_tensor[:, :, -1] = player_plane\n",
    "    return board_tensor"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_accel",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
